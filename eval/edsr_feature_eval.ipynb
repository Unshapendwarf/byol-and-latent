{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.jit' has no attribute 'unused'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2736503/3718981301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msqueezenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInception3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     def __init__(\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mInception3\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munused\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meager_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.jit' has no attribute 'unused'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import yaml\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from torch import cuda\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from mymodels.resnet_base_network import ResNet18\n",
    "from mydata.imageloader import MyDataset, psnrDataUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_transforms = torchvision.transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"../config/config.yaml\", \"r\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating index dict\n",
    "import random\n",
    "import glob\n",
    "\n",
    "indexDict = dict()\n",
    "file_path = \"/home/hong/dir1/PyTorch-BYOL/writing2.txt\"\n",
    "# open index file\n",
    "f = open(file_path, 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    listed = line.split(\" \")  # split by spaces\n",
    "    if listed[0] not in indexDict:\n",
    "        # dict has img1_name\n",
    "        indexDict[listed[0]] = list()\n",
    "    tmp_unit = psnrDataUnit(listed[1], listed[2], listed[3])\n",
    "    indexDict[listed[0]].append(tmp_unit)\n",
    "f.close()\n",
    "# check the loading is valid or not\n",
    "indexDictKeys = list(indexDict.keys())\n",
    "random.shuffle(indexDictKeys) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_image_path example:  ('/mnt/URP_DS/HR/44_22.png', 51.27391754) ('/mnt/URP_DS/HR/67_6.png', 44.79727264)\n",
      "1613\n"
     ]
    }
   ],
   "source": [
    "image_path = '/mnt/URP_DS/HR' # without last slash\n",
    "test_data_path = '/mnt/URP_DS/HR'  #without last slash\n",
    "\n",
    "img1_idx = 0\n",
    "\n",
    "train_img_psnr_list = []\n",
    "for p_unit in indexDict[indexDictKeys[img1_idx]]:\n",
    "    train_img_psnr_list.append((image_path + '/'+p_unit.getimg2()+'.png', p_unit.getonetwo()))\n",
    "# train_image_paths= np.array(train_image_paths).flatten().tolist()\n",
    "# random.shuffle(train_image_paths)  # ??? necessary??\n",
    "\n",
    "print('train_image_path example: ', train_img_psnr_list[0], train_img_psnr_list[10])\n",
    "print(len(train_img_psnr_list))\n",
    "\n",
    "# split train valid from train paths (80,20)\n",
    "# train_img_psnr_list, valid_img_psnr_list = train_img_psnr_list[:int(0.8*len(train_img_psnr_list))], train_img_psnr_list[int(0.8*len(train_img_psnr_list)):]\n",
    "\n",
    "# create the test_image_paths\n",
    "test_img_psnr_list = []\n",
    "for p_unit in indexDict[indexDictKeys[img1_idx]]:\n",
    "    test_img_psnr_list.append((image_path + '/'+p_unit.getimg2()+'.png', p_unit.getonetwo()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cpu\n"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"Training with: {device}\")\n",
    "\n",
    "train_dataset = MyDataset(train_img_psnr_list, transform=data_transforms)\n",
    "# test_dataset = MyDataset(test_img_psnr_list, transform=data_transforms)\n",
    "test_dataset = MyDataset(train_img_psnr_list, transform=data_transforms)\n",
    "single_dataset = MyDataset([('/mnt/URP_DS/HR/'+indexDictKeys[img1_idx]+'.png', 40)], transform=data_transforms)  # 40-> meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape:\", train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          num_workers=0, drop_last=False, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                          num_workers=0, drop_last=False, shuffle=True, pin_memory=True)\n",
    "                          \n",
    "single_loader = DataLoader(single_dataset, batch_size=1,\n",
    "                          num_workers=0, drop_last=False, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "encoder = ResNet18(**config['network'])\n",
    "output_feature_dim = encoder.projetion.net[0].in_features\n",
    "print(output_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "#load pre-trained parameters\n",
    "#load_params = torch.load(os.path.join('/home/hong/dir1/PyTorch-BYOL/runs/Jul29_18-37-02_mango2/checkpoints/model.pth'),\n",
    "#                         map_location=torch.device(torch.device(device)))\n",
    "# epoch: 10\n",
    "load_params = torch.load(os.path.join('/home/hong/dir1/PyTorch-BYOL/runs/Sep26_15-10-29_mango2/checkpoints/model.pth'),\n",
    "                        map_location=torch.device(torch.device(device)))\n",
    "\n",
    "#load_params = torch.load(os.path.join('/home/hong/dir1/PyTorch-BYOL/runs/Jul29_01-32-09_mango2/checkpoints/model.pth'),\n",
    "#                         map_location=torch.device(torch.device(device)))\\\n",
    "\n",
    "# hong2\n",
    "load_params2 = torch.load(os.path.join('/home/hong/dir1/PyTorch-BYOL/runs/using0928/checkpoints/model.pth'),\n",
    "                        map_location=torch.device(torch.device(device)))\n",
    "\n",
    "if 'online_network_state_dict' in load_params:\n",
    "    encoder.load_state_dict(load_params['online_network_state_dict'])\n",
    "    print(\"Parameters successfully loaded.\")\n",
    "\n",
    "# remove the projection head\n",
    "encoder = torch.nn.Sequential(*list(encoder.children())[:-1])    \n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n",
    "class MyOne(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, H3)\n",
    "        self.linear4 = torch.nn.Linear(H3, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        x = torch.nn.functional.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([512, 1024])\n",
      "linear1.bias \t torch.Size([512])\n",
      "linear2.weight \t torch.Size([256, 512])\n",
      "linear2.bias \t torch.Size([256])\n",
      "linear3.weight \t torch.Size([64, 256])\n",
      "linear3.bias \t torch.Size([64])\n",
      "linear4.weight \t torch.Size([1, 64])\n",
      "linear4.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(output_feature_dim*2, 10)\n",
    "# logreg = logreg.to(device)\n",
    "mymo = MyOne(output_feature_dim*2, 512, 256, 64, 1)\n",
    "mymo = mymo.to(device)\n",
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in mymo.state_dict():\n",
    "    print(param_tensor, \"\\t\", mymo.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_encoder(encoder, loader):\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # get the features from the pre-trained model\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        print(x.shape)\n",
    "        with torch.no_grad():\n",
    "            feature_vector = encoder(x)\n",
    "            x_train.extend(feature_vector)\n",
    "            y_train.extend(y.numpy())\n",
    "\n",
    "            \n",
    "    x_train = torch.stack(x_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 512, 512])\n",
      "torch.Size([64, 3, 512, 512])\n",
      "torch.Size([64, 3, 512, 512])\n",
      "torch.Size([64, 3, 512, 512])\n",
      "torch.Size([64, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "x_train, y_train = get_features_from_encoder(encoder, train_loader)\n",
    "x_test, y_test = get_features_from_encoder(encoder, test_loader)\n",
    "\n",
    "if len(x_train.shape) > 2:\n",
    "    print(x_train.shape)\n",
    "    x_train = torch.mean(x_train, dim=[2, 3])\n",
    "    x_test = torch.mean(x_test, dim=[2, 3])\n",
    "    \n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1317, 512])\n",
      "torch.Size([330, 512])\n",
      "torch.Size([1317, 1024]) torch.Size([330, 1024])\n"
     ]
    }
   ],
   "source": [
    "x_single, y_single = get_features_from_encoder(encoder, single_loader)\n",
    "single_list = []\n",
    "for i in range (0,x_train.shape[0]):\n",
    "    single_list.append(x_single)\n",
    "x_single1 = torch.stack(single_list, 1)\n",
    "x_single1 = x_single1.squeeze()\n",
    "print(x_single1.shape)\n",
    "\n",
    "x_train = torch.cat((x_train, x_single1), 1)\n",
    "\n",
    "single_list.clear()\n",
    "for i in range (0,x_test.shape[0]):\n",
    "    single_list.append(x_single)\n",
    "x_single2 = torch.stack(single_list, 1)\n",
    "x_single2 = x_single2.squeeze()\n",
    "print(x_single2.shape)\n",
    "x_test = torch.cat((x_test, x_single2), 1)\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_arrays(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train).astype(np.float32)\n",
    "x_test = scaler.transform(x_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_data_loaders_from_arrays(torch.from_numpy(x_train), y_train, torch.from_numpy(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hong/anaconda3/envs/cuda11/lib/python3.8/site-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/hong/anaconda3/envs/cuda11/lib/python3.8/site-packages/torch/nn/modules/loss.py:97: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 40.154814, min: 39.489699, avg: 39.859973\n",
      "max: 40.358824, min: 40.099970, avg: 40.220761\n",
      "max: 40.650260, min: 40.385801, avg: 40.546250\n",
      "max: 40.802886, min: 40.683036, avg: 40.760205\n",
      "max: 41.343215, min: 41.066036, avg: 41.267908\n",
      "max: 40.445629, min: 40.176594, avg: 40.356597\n",
      "max: 40.449566, min: 40.133184, avg: 40.349050\n",
      "max: 40.056892, min: 39.548082, avg: 39.874135\n",
      "max: 40.678843, min: 40.504375, avg: 40.588872\n",
      "max: 40.225704, min: 40.111145, avg: 40.187753\n",
      "max: 40.647847, min: 40.527827, avg: 40.605909\n",
      "max: 40.040186, min: 39.807670, avg: 39.953034\n",
      "max: 40.678313, min: 40.376155, avg: 40.579920\n",
      "max: 40.065867, min: 40.025631, avg: 40.050479\n",
      "max: 40.352265, min: 39.980831, avg: 40.199831\n",
      "max: 39.974359, min: 39.761159, avg: 39.841047\n",
      "max: 41.044185, min: 40.847398, avg: 40.979551\n",
      "max: 40.918891, min: 40.877143, avg: 40.897500\n",
      "max: 40.710467, min: 40.587911, avg: 40.628555\n",
      "max: 39.676947, min: 39.456167, avg: 39.578801\n"
     ]
    }
   ],
   "source": [
    "running_loss_history = []\n",
    "running_logit_history = []\n",
    "\n",
    "optimizer = torch.optim.Adam(mymo.parameters(), lr=3e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.L1Loss()\n",
    "eval_every_n_epochs = 10\n",
    "\n",
    "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# print(f\"Training with: {device}\")\n",
    "\n",
    "for epoch in range(200):\n",
    "#     train_acc = []\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()        \n",
    "        \n",
    "        logits = mymo(x)\n",
    "        running_logit_history.append(1/logits[0].item())\n",
    "        # predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        loss = criterion(logits, y)\n",
    "        running_loss_history.append(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # total = 0\n",
    "    if epoch % eval_every_n_epochs == 0:\n",
    "        # correct = 0\n",
    "        validation_psnrsum_history = []\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = mymo(x)\n",
    "            validation_psnrsum_history.append(1/logits[0].item()-1/y[0].item()])\n",
    "            # predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # total += y.size(0)\n",
    "            # correct += (predictions == y).sum().item()\n",
    "        mymax = max(validation_psnrsum_history)\n",
    "        mymin = min(validation_psnrsum_history)\n",
    "        avg = sum(validation_psnrsum_history, 0.0)/len(validation_psnrsum_history)\n",
    "        prediction = (mymax, mymin, avg)\n",
    "        # acc = 100 * correct / total\n",
    "        print(\"max: %f, min: %f, avg: %f\" % prediction)\n",
    "        print(validation_psnrsum_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.98486508538505, 40.758923225888644, 40.6069691960277, 40.49976750410451]\n",
      "4200\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(running_logit_history[-4:])\n",
    "print(len(running_logit_history))\n",
    "print(len(validation_psnrsum_history))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4065fc4c518883b19d5c7146216b3edbe965f2c3fa5af4d588617880bfa064c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('october': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
