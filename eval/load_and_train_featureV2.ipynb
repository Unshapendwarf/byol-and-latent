{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package, module setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from torch import cuda\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from mymodels.resnet_base_network import ResNet18\n",
    "from mydata.imageloader import MyDataset, psnrDataUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "data_transforms = torchvision.transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "config = yaml.load(open(\"../config/config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"Training with: {device}\")\n",
    "if device=='cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-data, Test-data\n",
    "- shuffle the train data\n",
    "- dataset\n",
    "- dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder loading\n",
    "load encoder for both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "encoder = ResNet18(**config['network'])\n",
    "output_feature_dim = encoder.projetion.net[0].in_features\n",
    "print(output_feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load encoded tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([101463, 512]) torch.Size([101463, 512]) torch.Size([101463])\n",
      "Testing data shape: torch.Size([101463, 512]) torch.Size([101463, 512]) torch.Size([101463])\n"
     ]
    }
   ],
   "source": [
    "# load tensor array\n",
    "train_load_path = \"./tensors/run1111_0505/train.pt\"\n",
    "test_load_path = \"./tensors/run1111_0505/test.pt\"\n",
    "\n",
    "# train_load_path = \"./tensors/run1111_0505/train.pt\"  # 100,000 data\n",
    "# test_load_path = \"./tensors/run1111_0505/test.pt\"   # 100,000 data\n",
    "\n",
    "loaded1 = torch.load(train_load_path)\n",
    "loaded2 = torch.load(test_load_path)\n",
    "\n",
    "x1_train = loaded1['x1']\n",
    "x2_train = loaded1['x2']\n",
    "y_train = loaded1['y']\n",
    "x1_test = loaded2['x1']\n",
    "x2_test = loaded2['x2']\n",
    "y_test = loaded2['y']\n",
    "\n",
    "print(\"Training data shape:\", x1_train.shape, x2_train.shape, y_train.shape)\n",
    "print(\"Testing data shape:\", x1_test.shape, x2_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x1_train)\n",
    "x1_train = scaler.transform(x1_train).astype(np.float32)\n",
    "x2_train = scaler.transform(x2_train).astype(np.float32)\n",
    "x1_test = scaler.transform(x1_test).astype(np.float32)\n",
    "x2_test = scaler.transform(x2_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders_from_arrays(X1_train, X2_train, y_train, X1_test, X2_test, y_test):\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(X1_train, X2_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=512, shuffle=True)\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(X1_test, X2_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = create_data_loaders_from_arrays(torch.from_numpy(x1_train), torch.from_numpy(x2_train), y_train, \\\n",
    "                                                            torch.from_numpy(x1_test), torch.from_numpy(x2_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP setting\n",
    "number of hidden layers: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n",
    "class MyOne(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, H4, H5, H6, H7, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, H3)\n",
    "        self.linear4 = torch.nn.Linear(H3, H4)\n",
    "        self.linear5 = torch.nn.Linear(H4, H5)\n",
    "        self.linear6 = torch.nn.Linear(H5, H6)\n",
    "        self.linear7 = torch.nn.Linear(H6, H7)\n",
    "        self.linear8 = torch.nn.Linear(H7, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        x = torch.nn.functional.relu(self.linear3(x))\n",
    "        x = torch.nn.functional.relu(self.linear4(x))\n",
    "        x = torch.nn.functional.relu(self.linear5(x))\n",
    "        x = torch.nn.functional.relu(self.linear6(x))\n",
    "        x = torch.nn.functional.relu(self.linear7(x))\n",
    "        x = self.linear8(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastOne(torch.nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([512, 512])\n",
      "linear1.bias \t torch.Size([512])\n",
      "linear2.weight \t torch.Size([512, 512])\n",
      "linear2.bias \t torch.Size([512])\n",
      "linear3.weight \t torch.Size([256, 512])\n",
      "linear3.bias \t torch.Size([256])\n",
      "linear4.weight \t torch.Size([256, 256])\n",
      "linear4.bias \t torch.Size([256])\n",
      "linear5.weight \t torch.Size([128, 256])\n",
      "linear5.bias \t torch.Size([128])\n",
      "linear6.weight \t torch.Size([128, 128])\n",
      "linear6.bias \t torch.Size([128])\n",
      "linear7.weight \t torch.Size([64, 128])\n",
      "linear7.bias \t torch.Size([64])\n",
      "linear8.weight \t torch.Size([64, 64])\n",
      "linear8.bias \t torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(output_feature_dim*2, 10)\n",
    "# logreg = logreg.to(device)\n",
    "# mymo = MyOne(output_feature_dim, output_feature_dim, output_feature_dim, output_feature_dim, output_feature_dim, output_feature_dim, 256)\n",
    "\n",
    "# hidden 7\n",
    "mymo = MyOne(output_feature_dim, 512, 512, 256, 256, 128, 128, 64, 64)\n",
    "mymo = mymo.to(device)\n",
    "\n",
    "# # hidden 3\n",
    "# mymo = MyOne(output_feature_dim, 256, 128, 64, 16)\n",
    "# mymo = mymo.to(device)\n",
    "\n",
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in mymo.state_dict():\n",
    "    print(param_tensor, \"\\t\", mymo.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91317/2948716950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# loss = criterion(logits.squeeze(1), y.unsqueeze(1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/october/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(mymo.parameters(), lr=3e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.L1Loss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "eval_every_n_epochs = 20\n",
    "first_epoch = 400\n",
    "mymo.train()\n",
    "temp1, temp2, temp3 = 1, 2, 3\n",
    "check = 1;\n",
    "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "# print(f\"Training with: {device}\")\n",
    "for epoch in tqdm(range(first_epoch)):\n",
    "#     train_acc = []\n",
    "    for x1, x2, y in train_loader:\n",
    "\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        temp1 = x1\n",
    "        temp2 = x2\n",
    "        temp3 = y\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        out1 = mymo(x1)\n",
    "        out2 = mymo(x2)\n",
    "        # predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # out = torch.cat([out1, out2], dim=1)\n",
    "        # logits = lastlayer(out)\n",
    "        logits = torch.cdist(out1.unsqueeze(1), out2.unsqueeze(1))\n",
    "        \n",
    "        # if epoch % eval_every_n_epochs == 0 and check == 1:\n",
    "        #     print(temp1.shape, temp2.shape, temp3.shape, out1.shape, out.shape)\n",
    "        #     print(logits.squeeze(1).shape)\n",
    "        #     print(y.unsqueeze(1).shape)\n",
    "        #     check = 0\n",
    "\n",
    "\n",
    "        loss = criterion(logits.squeeze(1), y.unsqueeze(1))\n",
    "        # loss = criterion(logits, y.unsqueeze(1))\n",
    "        # loss = criterion(logits.squeeze(1), y.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_diff = []\n",
    "\n",
    "now = time.localtime()\n",
    "\n",
    "with torch.no_grad():\n",
    "    mymo.eval()\n",
    "    for x1, x2, y in tqdm(test_loader):\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)    \n",
    "        \n",
    "        out1 = mymo(x1)\n",
    "        out2 = mymo(x2)\n",
    "        # predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        logits = torch.cdist(out1.unsqueeze(1), out2.unsqueeze(1))\n",
    "        logits = logits.squeeze(1)\n",
    "\n",
    "        ty = 1/float(y[0].item())\n",
    "        \n",
    "        if logits[0].item()==0:\n",
    "            oy = 100\n",
    "        else: \n",
    "            oy = 1 /logits[0].item()\n",
    "        test_result_diff.append((ty, oy, abs(ty-oy)))\n",
    "\n",
    "with open(\"result_%.2f_%02d%02d_%02d%02d.txt\" %(1.0, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min), 'w') as f:\n",
    "    for item1, item2, item3 in test_result_diff:\n",
    "        f.write(\"sr_sum:%s, model_out:%s, diff(abs):%s\\n\" % (item1, item2, item3))    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4065fc4c518883b19d5c7146216b3edbe965f2c3fa5af4d588617880bfa064c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('october': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
